{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# dt_rf_task.py\n",
        "\"\"\"\n",
        "Decision Trees & Random Forests - end-to-end example\n",
        "Save as: dt_rf_task.py\n",
        "Run: python dt_rf_task.py\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import joblib\n",
        "\n",
        "OUTPUT_DIR = \"outputs\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "def load_data(local_path=\"data/heart.csv\"):\n",
        "    \"\"\"\n",
        "    Try to load a CSV at local_path. If not found, fallback to sklearn breast cancer dataset.\n",
        "    \"\"\"\n",
        "    if os.path.exists(local_path):\n",
        "        print(f\"[INFO] Loading dataset from {local_path}\")\n",
        "        df = pd.read_csv(local_path)\n",
        "        # try to detect target column (common names)\n",
        "        for cand in [\"target\", \"Outcome\", \"HeartDisease\", \"heart_disease\", \"has_disease\"]:\n",
        "            if cand in df.columns:\n",
        "                target_col = cand\n",
        "                break\n",
        "        else:\n",
        "            # assume last column is target\n",
        "            target_col = df.columns[-1]\n",
        "        return df, target_col\n",
        "    else:\n",
        "        print(f\"[WARN] No local CSV at {local_path}. Falling back to sklearn breast_cancer dataset.\")\n",
        "        data = load_breast_cancer()\n",
        "        df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "        df[\"target\"] = data.target\n",
        "        return df, \"target\"\n",
        "\n",
        "\n",
        "def preprocess(df, target_col):\n",
        "    # Basic cleanup: drop duplicates, drop rows with missing target\n",
        "    df = df.copy()\n",
        "    df = df.drop_duplicates()\n",
        "    df = df.dropna(subset=[target_col])\n",
        "    # If any non-numeric columns (except target), one-hot encode them\n",
        "    X = df.drop(columns=[target_col])\n",
        "    y = df[target_col]\n",
        "    non_numeric = X.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
        "    if non_numeric:\n",
        "        X = pd.get_dummies(X, columns=non_numeric, drop_first=True)\n",
        "    return X, y\n",
        "\n",
        "\n",
        "def train_decision_tree(X_train, y_train, X_test, y_test, max_depth=None, random_state=42):\n",
        "    clf = DecisionTreeClassifier(max_depth=max_depth, random_state=random_state)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    print(f\"[DT] max_depth={max_depth}  test_acc={acc:.4f}\")\n",
        "    return clf, acc, y_pred\n",
        "\n",
        "\n",
        "def analyze_overfitting(X_train, X_test, y_train, y_test, depths=range(1, 16)):\n",
        "    train_scores = []\n",
        "    test_scores = []\n",
        "    for d in depths:\n",
        "        clf = DecisionTreeClassifier(max_depth=d, random_state=42)\n",
        "        clf.fit(X_train, y_train)\n",
        "        train_scores.append(clf.score(X_train, y_train))\n",
        "        test_scores.append(clf.score(X_test, y_test))\n",
        "\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(list(depths), train_scores, marker='o', label='train_accuracy')\n",
        "    plt.plot(list(depths), test_scores, marker='o', label='test_accuracy')\n",
        "    plt.xlabel(\"max_depth\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.title(\"Decision Tree: Train vs Test accuracy by max_depth\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    out_path = os.path.join(OUTPUT_DIR, \"dt_overfitting_depths.png\")\n",
        "    plt.savefig(out_path)\n",
        "    print(f\"[INFO] Saved overfitting plot to {out_path}\")\n",
        "    plt.close()\n",
        "\n",
        "    return depths, train_scores, test_scores\n",
        "\n",
        "\n",
        "def visualize_tree(clf, feature_names, class_names=None, max_display_depth=3, out_fname=\"dt_tree.png\"):\n",
        "    plt.figure(figsize=(20, 10))\n",
        "    plot_tree(clf,\n",
        "              feature_names=feature_names,\n",
        "              class_names=[str(c) for c in class_names] if class_names is not None else None,\n",
        "              filled=True,\n",
        "              max_depth=max_display_depth,\n",
        "              fontsize=8)\n",
        "    plt.title(\"Decision Tree (top levels)\")\n",
        "    plt.tight_layout()\n",
        "    out_path = os.path.join(OUTPUT_DIR, out_fname)\n",
        "    plt.savefig(out_path)\n",
        "    print(f\"[INFO] Saved tree visualization to {out_path}\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def train_random_forest(X_train, y_train, X_test, y_test, n_estimators=100, random_state=42):\n",
        "    rf = RandomForestClassifier(n_estimators=n_estimators, random_state=random_state, n_jobs=-1)\n",
        "    rf.fit(X_train, y_train)\n",
        "    y_pred = rf.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    print(f\"[RF] n_estimators={n_estimators}  test_acc={acc:.4f}\")\n",
        "    return rf, acc, y_pred\n",
        "\n",
        "\n",
        "def show_feature_importances(model, feature_names, top_k=15, prefix=\"model\"):\n",
        "    importances = model.feature_importances_\n",
        "    idx = np.argsort(importances)[::-1][:top_k]\n",
        "    top_feats = [(feature_names[i], importances[i]) for i in idx]\n",
        "\n",
        "    # DataFrame\n",
        "    df_imp = pd.DataFrame(top_feats, columns=[\"feature\", \"importance\"])\n",
        "    print(f\"\\nTop {top_k} feature importances for {prefix}:\")\n",
        "    print(df_imp.to_string(index=False))\n",
        "\n",
        "    # plot\n",
        "    plt.figure(figsize=(8, max(3, 0.25 * top_k)))\n",
        "    plt.barh(df_imp[\"feature\"][::-1], df_imp[\"importance\"][::-1])\n",
        "    plt.xlabel(\"Importance\")\n",
        "    plt.title(f\"Top {top_k} feature importances ({prefix})\")\n",
        "    out_path = os.path.join(OUTPUT_DIR, f\"{prefix}_feature_importances.png\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path)\n",
        "    print(f\"[INFO] Saved feature importance plot to {out_path}\")\n",
        "    plt.close()\n",
        "    return df_imp\n",
        "\n",
        "\n",
        "def evaluate_and_report(y_test, y_pred, label=\"Model\"):\n",
        "    print(f\"\\n{label} - Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "    print(\"Classification report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    print(\"Confusion matrix:\")\n",
        "    print(cm)\n",
        "\n",
        "\n",
        "def cross_val_estimate(model, X, y, cv=5):\n",
        "    scores = cross_val_score(model, X, y, cv=cv, n_jobs=-1)\n",
        "    print(f\"[CROSS-VAL] mean_acc={scores.mean():.4f}  std={scores.std():.4f}\")\n",
        "    return scores\n",
        "\n",
        "\n",
        "def main():\n",
        "    df, target_col = load_data(local_path=\"data/heart.csv\")\n",
        "    print(f\"[INFO] Data shape: {df.shape}; target_col: {target_col}\")\n",
        "\n",
        "    X, y = preprocess(df, target_col)\n",
        "    feature_names = X.columns.tolist()\n",
        "\n",
        "    # Train/test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.20, random_state=42, stratify=y if len(np.unique(y))>1 else None\n",
        "    )\n",
        "    print(f\"[INFO] Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
        "\n",
        "    # Baseline Decision Tree\n",
        "    dt_clf, dt_acc, dt_pred = train_decision_tree(X_train, y_train, X_test, y_test, max_depth=None)\n",
        "    evaluate_and_report(y_test, dt_pred, label=\"Decision Tree (unrestricted)\")\n",
        "\n",
        "    # Visualize top of tree\n",
        "    visualize_tree(dt_clf, feature_names=feature_names, class_names=np.unique(y), max_display_depth=3,\n",
        "                   out_fname=\"dt_tree_top3.png\")\n",
        "\n",
        "    # Overfitting analysis\n",
        "    depths, train_scores, test_scores = analyze_overfitting(X_train, X_test, y_train, y_test, depths=range(1, 16))\n",
        "\n",
        "    # Cross-validation estimate for the best simple tree (we'll pick depth where test accuracy peaked)\n",
        "    best_depth = int(np.array(depths)[np.argmax(test_scores)])\n",
        "    print(f\"[INFO] Best max_depth by test set within sampled range: {best_depth}\")\n",
        "    dt_best = DecisionTreeClassifier(max_depth=best_depth, random_state=42)\n",
        "    cross_val_estimate(dt_best, X, y, cv=5)\n",
        "\n",
        "    # Random Forest\n",
        "    rf_clf, rf_acc, rf_pred = train_random_forest(X_train, y_train, X_test, y_test, n_estimators=200)\n",
        "    evaluate_and_report(y_test, rf_pred, label=\"Random Forest\")\n",
        "\n",
        "    cross_val_estimate(RandomForestClassifier(n_estimators=200, random_state=42), X, y, cv=5)\n",
        "\n",
        "    # Feature importances\n",
        "    show_feature_importances(dt_clf, feature_names, top_k=min(15, len(feature_names)), prefix=\"decision_tree\")\n",
        "    show_feature_importances(rf_clf, feature_names, top_k=min(15, len(feature_names)), prefix=\"random_forest\")\n",
        "\n",
        "    # Save models\n",
        "    joblib.dump(dt_clf, os.path.join(OUTPUT_DIR, \"decision_tree_model.joblib\"))\n",
        "    joblib.dump(rf_clf, os.path.join(OUTPUT_DIR, \"random_forest_model.joblib\"))\n",
        "    print(f\"[INFO] Saved models to {OUTPUT_DIR}\")\n",
        "\n",
        "    # Save a small summary file\n",
        "    with open(os.path.join(OUTPUT_DIR, \"summary.txt\"), \"w\") as f:\n",
        "        f.write(\"Decision Tree test accuracy: {:.4f}\\n\".format(dt_acc))\n",
        "        f.write(\"Random Forest test accuracy: {:.4f}\\n\".format(rf_acc))\n",
        "        f.write(\"Best max_depth (from sampled range): {}\\n\".format(best_depth))\n",
        "    print(f\"[INFO] Summary saved to {os.path.join(OUTPUT_DIR, 'summary.txt')}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MD7EXSykJxNW",
        "outputId": "8bb3945d-e19b-42ca-f885-cdb2611920fe"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[WARN] No local CSV at data/heart.csv. Falling back to sklearn breast_cancer dataset.\n",
            "[INFO] Data shape: (569, 31); target_col: target\n",
            "[INFO] Train shape: (455, 30), Test shape: (114, 30)\n",
            "[DT] max_depth=None  test_acc=0.9123\n",
            "\n",
            "Decision Tree (unrestricted) - Accuracy: 0.9123\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.93      0.89        42\n",
            "           1       0.96      0.90      0.93        72\n",
            "\n",
            "    accuracy                           0.91       114\n",
            "   macro avg       0.90      0.92      0.91       114\n",
            "weighted avg       0.92      0.91      0.91       114\n",
            "\n",
            "Confusion matrix:\n",
            "[[39  3]\n",
            " [ 7 65]]\n",
            "[INFO] Saved tree visualization to outputs/dt_tree_top3.png\n",
            "[INFO] Saved overfitting plot to outputs/dt_overfitting_depths.png\n",
            "[INFO] Best max_depth by test set within sampled range: 3\n",
            "[CROSS-VAL] mean_acc=0.9191  std=0.0246\n",
            "[RF] n_estimators=200  test_acc=0.9561\n",
            "\n",
            "Random Forest - Accuracy: 0.9561\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.93      0.94        42\n",
            "           1       0.96      0.97      0.97        72\n",
            "\n",
            "    accuracy                           0.96       114\n",
            "   macro avg       0.96      0.95      0.95       114\n",
            "weighted avg       0.96      0.96      0.96       114\n",
            "\n",
            "Confusion matrix:\n",
            "[[39  3]\n",
            " [ 2 70]]\n",
            "[CROSS-VAL] mean_acc=0.9578  std=0.0238\n",
            "\n",
            "Top 15 feature importances for decision_tree:\n",
            "                feature  importance\n",
            "           worst radius    0.697811\n",
            "   worst concave points    0.116083\n",
            "          worst texture    0.058881\n",
            "          texture error    0.052661\n",
            "        worst concavity    0.016325\n",
            "             worst area    0.012678\n",
            "             area error    0.012085\n",
            "         worst symmetry    0.010733\n",
            "       worst smoothness    0.008452\n",
            "   concave points error    0.006261\n",
            "    mean concave points    0.006261\n",
            "       smoothness error    0.001770\n",
            "      worst compactness    0.000000\n",
            "worst fractal dimension    0.000000\n",
            "        concavity error    0.000000\n",
            "[INFO] Saved feature importance plot to outputs/decision_tree_feature_importances.png\n",
            "\n",
            "Top 15 feature importances for random_forest:\n",
            "             feature  importance\n",
            "     worst perimeter    0.133100\n",
            "          worst area    0.128052\n",
            "worst concave points    0.108107\n",
            " mean concave points    0.094414\n",
            "        worst radius    0.090639\n",
            "         mean radius    0.058662\n",
            "      mean perimeter    0.055242\n",
            "           mean area    0.049938\n",
            "      mean concavity    0.046207\n",
            "     worst concavity    0.035357\n",
            "          area error    0.034368\n",
            "    mean compactness    0.018094\n",
            "       worst texture    0.017869\n",
            "   worst compactness    0.014481\n",
            "        mean texture    0.014317\n",
            "[INFO] Saved feature importance plot to outputs/random_forest_feature_importances.png\n",
            "[INFO] Saved models to outputs\n",
            "[INFO] Summary saved to outputs/summary.txt\n"
          ]
        }
      ]
    }
  ]
}